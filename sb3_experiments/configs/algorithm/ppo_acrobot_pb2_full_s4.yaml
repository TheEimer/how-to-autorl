agent_class: PPO
total_timesteps: 1e6
n_eval_episodes: 5
policy_model: MlpPolicy
num_minibatches: 8
model_kwargs:
  learning_rate: [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,3.358400656135759e-06,3.358400656135759e-06]
  batch_size: [16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,32,32]
  gamma: 0.9
  n_steps: [512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,512,1024,1024]
  n_epochs: [13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,8,8]
  gae_lambda: [0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.9999,0.9999]
  clip_range: [0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.2742952893047171,0.2742952893047171]
  clip_range_vf: null
  normalize_advantage: False
  ent_coef: [0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.3904919432393234,0.4982915708545033,0.4982915708545033]
  vf_coef: [0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.7518727521626938,0.9058722203388524,0.9058722203388524]
  max_grad_norm: [0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.30026865375,0.9496383180436271,0.9496383180436271]
  use_sde: False
  sde_sample_freq: 4