agent_class: PPO
total_timesteps: 1e6
n_eval_episodes: 5
policy_model: MlpPolicy
num_minibatches: 8
model_kwargs:
  learning_rate: [1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.6595328074164297e-05,1.535277604725449e-06,1.535277604725449e-06]
  batch_size: 64
  gamma: 0.9
  n_steps: 1024
  n_epochs: [16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,15,15]
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  normalize_advantage: True
  ent_coef: [0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.25600000000000006,0.176645856864158,0.176645856864158,]
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: 4