agent_class: SAC
total_timesteps: 1e6
n_eval_episodes: 5
policy_model: MlpPolicy
model_kwargs:
  learning_rate: [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,5.608919147521798e-06,5.608919147521798e-06]
  batch_size: 128
  tau: [0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,0.3572143124381274,1.0,1.0]
  gamma: 0.99
  learning_starts: [6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,6759,2707,2707]
  buffer_size: [11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,11460655,50000000,50000000]
  train_freq: [192,192,192,192,192,192,192,192,192,192,192,192,192,192,192,192,192,192,192,813,813]
  gradient_steps: [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,9,9]
  use_sde: False
  sde_sample_freq: -1