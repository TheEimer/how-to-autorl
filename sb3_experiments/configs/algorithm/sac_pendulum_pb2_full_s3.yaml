agent_class: SAC
total_timesteps: 1e6
n_eval_episodes: 5
policy_model: MlpPolicy
model_kwargs:
  learning_rate: [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00017853354625681206,0.00017853354625681206,0.00017853354625681206]
  batch_size: [64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,512,512,512,]
  tau: [0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.18474380944918944,0.5013074614471383,0.5013074614471383,0.5013074614471383]
  gamma: 0.99
  learning_starts: [1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,1867,5836,5836,5836]
  buffer_size: [48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,48000000,2404964,2404964,2404964]
  train_freq: [428,428,428,428,428,428,428,428,428,428,428,428,428,428,428,428,428,428,684,684,684]
  gradient_steps: [4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5]
  use_sde: False
  sde_sample_freq: -1